
[TOC]
## Java并发简介
> **关键词**:进程、线程、安全性、活跃性、性能、死锁、饥饿、上下文切换
> **摘要**:并发编程并非 Java 语言所独有，而是一种成熟的编程范式，Java 只是用自己的方式实现了并发工作模型。学习 Java 并发编程，应该先熟悉并发的基本概念，然后进一步了解并发的特性以及其特性所面临的问题。掌握了这些，当学习 Java 并发工具时，才会明白它们各自是为了解决什么问题，为什么要这样设计。通过这样由点到面的学习方式，更容易融会贯通，将并发知识形成体系化。
![56cbe86e94342421be308a6ad6623498.png](en-resource://database/768:1)@w=1340

### 并发编程
***

#### 并发和并行

* **并发**:是指具备处理多个任务的能力，但不一定要同时。
*  **并发**:是指具备同时处理多个任务的能力

#### 同步和异步 [（详细网站）](https://www.cnblogs.com/loveer/p/11479249.html)

* **同步**:是指在发出一个调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了
*  **异步**:则是相反，调用在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。

#### 阻塞和非阻塞

阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态：
* **阻塞**：是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。
* **非阻塞**：是指在不能立刻得到结果之前，该调用不会阻塞当前线程。

#### 进程和线程
* **进程**:进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动。进程是操作系统进行资源分配的基本单位。
* **线程**：线程是操作系统进行调度的基本单位。


进程和线程的差异：
* 一个程序至少有一个进程，一个进程至少有一个线程。
* 线程比进程划分更细，所以执行开销更小，并发性更高
* 进程是一个实体，拥有独立的资源；而同一个进程中的多个线程共享进程的资源。
  ![adeb10ed7b130013e9714fd7873af55f.png](en-resource://database/772:1)


JVM 在单个进程中运行，JVM 中的线程共享属于该进程的堆。这就是为什么几个线程可以访问同一个对象。线程共享堆并拥有自己的堆栈空间。这是一个线程如何调用一个方法以及它的局部变量是如何保持线程安全的。但是堆不是线程安全的并且为了线程安全必须进行同步。


#### 竞态条件和临界区
* 竞态条件（Race Condition）：当两个线程竞争同一资源时，如果对资源的访问顺序敏感，就称存在竞态条件。
* 临界区（Critical Sections）：导致竞态条件发生的代码区称作临界区。



#### 管程
管程（Monitor），是指管理共享变量以及对共享变量的操作过程，让他们支持并发。Java 采用的是管程技术，synchronized 关键字及 wait()、notify()、notifyAll() 这三个方法都是管程的组成部分。而**管程和信号量是等价的，所谓等价指的是用管程能够实现信号量，也能用信号量实现管程。**

### 并发的特点
*** 
因为计算机体系结构每个部件执行速度存在差异。

* **cpu增加了缓存**,以均衡与内存的速度差异。
* **操作系统增加了进程、线程**,一分时复用CPU劲儿均衡CPU与I/O设备的速度差异。
* **编译程序优化指令执行次序**,使得缓存能够得到更加合理地利用。


### 安全性问题
***
并发安全：是指保证程序的正确性，使得并发处理结果符合预期。

**并发安全**:是指保证程序的正确性，使得并发处理结果符合预期。

并发安全需要保证几个基本特性：
* 可见性：是一个线程修改某个共享变量，其状态能立即被其他线程知晓，通常被解释为将线程本地状态反映到主存上，volatile就是负责保证可见性的。
* 原子性：简单说就是相关操作不会中途被其他线程干扰，一般通过同步机制（加锁：sychronized、Lock)实现
* 有序性-是保证线程内串行语义、避免指令重排等。

##### 缓存导致的可见性问题
> 一个线程对共享变量的修改，另外一个线程能够立刻看到，称为可见性。


在单核时代，所有的线程都是在一颗 CPU 上执行，CPU 缓存与内存的数据一致性容易解决。因为所有线程都是操作同一个 CPU 的缓存，一个线程对缓存的写，对另外一个线程来说一定是可见的。例如在下面的图中，线程 A 和线程 B 都是操作同一个 CPU 里面的缓存，所以线程 A 更新了变量 V 的值，那么线程 B 之后再访问变量 V，得到的一定是 V 的最新值（线程 A 写过的值）。

![5b36425a306147348d1c00fd4780ff79.png](en-resource://database/778:1)


多核时代，每颗 CPU 都有自己的缓存，这时 CPU 缓存与内存的数据一致性就没那么容易解决了，当多个线程在不同的 CPU 上执行时，这些线程操作的是不同的 CPU 缓存。比如下图中，线程 A 操作的是 CPU-1 上的缓存，而线程 B 操作的是 CPU-2 上的缓存，很明显，这个时候线程 A 对变量 V 的操作对于线程 B 而言就不具备可见性了。
![2125d259079b048de7f8bb8e72016222.png](en-resource://database/780:1)


【示例】线程不安全的示例下面我们再用一段代码来验证一下多核场景下的可见性问题。下面的代码，每执行一次 add10K() 方法，都会循环 10000 次 count+=1 操作。在 calc() 方法中我们创建了两个线程，每个线程调用一次 add10K() 方法，我们来想一想执行 calc() 方法得到的结果应该是多少呢？
```java

public class Test {
  private long count = 0;
  private void add10K() {
    int idx = 0;
    while(idx++ < 10000) {
      count += 1;
    }
  }
  public static long calc() {
    final Test test = new Test();
    // 创建两个线程，执行 add() 操作
    Thread th1 = new Thread(()->{
      test.add10K();
    });
    Thread th2 = new Thread(()->{
      test.add10K();
    });
    // 启动两个线程
    th1.start();
    th2.start();
    // 等待两个线程执行结束
    th1.join();
    th2.join();
    return count;
  }
}

```

直觉告诉我们应该是 20000，因为在单线程里调用两次 add10K() 方法，count 的值就是 20000，但实际上 calc() 的执行结果是个 10000 到 20000 之间的随机数。为什么呢？

我们假设线程 A 和线程 B 同时开始执行，那么第一次都会将 count=0 读到各自的 CPU 缓存里，执行完 count+=1 之后，各自 CPU 缓存里的值都是 1，同时写入内存后，我们会发现内存中是 1，而不是我们期望的 2。之后由于各自的 CPU 缓存里都有了 count 的值，两个线程都是基于 CPU 缓存里的 count 值来计算，所以导致最终 count 的值都是小于 20000 的。这就是缓存的可见性问题。


##### 线程切换带来的原子性问题


由于 IO 太慢，早期的操作系统就发明了多进程，操作系统允许某个进程执行一小段时间（称为 时间片）。在一个时间片内，如果一个进程进行一个 IO 操作，例如读个文件，这个时候该进程可以把自己标记为“休眠状态”并出让 CPU 的使用权，待文件读进内存，操作系统会把这个休眠的进程唤醒，唤醒后的进程就有机会重新获得 CPU 的使用权了。这里的进程在等待 IO 时之所以会释放 CPU 使用权，是为了让 CPU 在这段等待时间里可以做别的事情，这样一来 CPU 的使用率就上来了；此外，如果这时有另外一个进程也读文件，读文件的操作就会排队，磁盘驱动在完成一个进程的读操作后，发现有排队的任务，就会立即启动下一个读操作，这样 IO 的使用率也上来了。早期的操作系统基于进程来调度 CPU，不同进程间是不共享内存空间的，所以进程要做任务切换就要切换内存映射地址，而一个进程创建的所有线程，都是共享一个内存空间的，所以线程做任务切换成本就很低了。现代的操作系统都基于更轻量的线程来调度，现在我们提到的“任务切换”都是指“线程切换”。Java 并发程序都是基于多线程的，自然也会涉及到任务切换，也许你想不到，任务切换竟然也是并发编程里诡异 Bug 的源头之一。任务切换的时机大多数是在时间片结束的时候，我们现在基本都使用高级语言编程，高级语言里一条语句往往需要多条 CPU 指令完成，例如上面代码中的 count += 1，至少需要三条 CPU 指令。
* 指令1：先把变量count从内存加载到cpu的寄存器
* 指令2：之后再cpu中进行运算
* 指令3：再把运算结果输出到内存中。


操作系统做任务切换，可以发生在任何一条CPU 指令执行完，是的，是 CPU 指令，而不是高级语言里的一条语句。对于上面的三条指令来说，我们假设 count=0，如果线程 A 在指令 1 执行完后做线程切换，线程 A 和线程 B 按照下图的序列执行，那么我们会发现两个线程都执行了 count+=1 的操作，但是得到的结果不是我们期望的 2，而是 1。
![f3ab0324ca0bcc83fb86a11fee2994b3.png](en-resource://database/782:1)


我们潜意识里面觉得 count+=1 这个操作是一个不可分割的整体，就像一个原子一样，线程的切换可以发生在 count+=1 之前，也可以发生在 count+=1 之后，但就是不会发生在中间。我们把一个或者多个操作在 CPU 执行的过程中不被中断的特性称为原子性。CPU 能保证的原子操作是 CPU 指令级别的，而不是高级语言的操作符，这是违背我们直觉的地方。因此，很多时候我们需要在高级语言层面保证操作的原子性。

##### 编译优化带来的有序性问题

那并发编程里还有没有其他有违直觉容易导致诡异 Bug 的技术呢？有的，就是有序性。顾名思义，有序性指的是程序按照代码的先后顺序执行。编译器为了优化性能，有时候会改变程序中语句的先后顺序，例如程序中：“a=6；b=7；”编译器优化后可能变成“b=7；a=6；”，在这个例子中，编译器调整了语句的顺序，但是不影响程序的最终结果。不过有时候编译器及解释器的优化可能导致意想不到的 Bug。


在 Java 领域一个经典的案例就是利用双重检查创建单例对象，例如下面的代码：在获取实例 getInstance() 的方法中，我们首先判断 instance 是否为空，如果为空，则锁定 Singleton.class 并再次检查 instance 是否为空，如果还为空则创建 Singleton 的一个实例


```java
public class Singleton {
  static Singleton instance;
  static Singleton getInstance(){
    if (instance == null) {
      synchronized(Singleton.class) {
        if (instance == null)
          instance = new Singleton();
        }
    }
    return instance;
  }
}
```


假设有两个线程 A、B 同时调用 getInstance() 方法，他们会同时发现 instance == null ，于是同时对 Singleton.class 加锁，此时 JVM 保证只有一个线程能够加锁成功（假设是线程 A），另外一个线程则会处于等待状态（假设是线程 B）；线程 A 会创建一个 Singleton 实例，之后释放锁，锁释放后，线程 B 被唤醒，线程 B 再次尝试加锁，此时是可以加锁成功的，加锁成功后，线程 B 检查 instance == null 时会发现，已经创建过 Singleton 实例了，所以线程 B 不会再创建一个 Singleton 实例。


这看上去一切都很完美，无懈可击，但实际上这个 getInstance() 方法并不完美。问题出在哪里呢？出在 new 操作上，我们以为的 new 操作应该是：

1. 分配一块内存 M；
2. 在内存 M 上初始化 Singleton 对象；
3. 然后 M 的地址赋值给 instance 变量。

但是实际上优化后的执行路径却是这样的：

1. 分配一块内存 M；
2. 将 M 的地址赋值给 instance 变量；
3. 最后在内存 M 上初始化 Singleton 对象。


优化后会导致什么问题呢？我们假设线程 A 先执行 getInstance() 方法，当执行完指令 2 时恰好发生了线程切换，切换到了线程 B 上；如果此时线程 B 也执行 getInstance() 方法，那么线程 B 在执行第一个判断时会发现 instance != null ，所以直接返回 instance，而此时的 instance 是没有初始化过的，如果我们这个时候访问 instance 的成员变量就可能触发空指针异常。
![c173a780bf4f949df29f018131419bce.png](en-resource://database/784:1)

##### 保证并发安全的思路
###### 互斥同步（阻塞同步）
互斥同步是最常见的并发正确性保障手段。
**是指多线程并发访问共享数据时，保证共享数据在同一时刻只能被一个线程访问**

互斥是实现同步的一种手段。临界区（Critical Sections）、互斥量（Mutex）和信号量（Semaphore）都是主要的互斥实现方式。

最典型的案例是使用 synchronized 或 Lock 。


互斥同步最主要的问题是线程阻塞和唤醒所带来的性能问题，互斥同步属于一种悲观的并发策略，总是认为只要不去做正确的同步措施，那就肯定会出现问题。无论共享数据是否真的会出现竞争，它都要进行加锁（这里讨论的是概念模型，实际上虚拟机会优化掉很大一部分不必要的加锁）、用户态核心态转换、维护锁计数器和检查是否有被阻塞的线程需要唤醒等操作。
###### 非阻塞同步

随着硬件指令集的发展，我们可以使用基于冲突检测的乐观并发策略：先进行操作，如果没有其它线程争用共享数据，那操作就成功了，否则采取补偿措施（不断地重试，直到成功为止）。这种乐观的并发策略的许多实现都不需要将线程阻塞，因此这种同步操作称为非阻塞同步。


为什么说乐观锁需要 硬件指令集的发展 才能进行？因为需要操作和冲突检测这两个步骤具备原子性。而这点是由硬件来完成，如果再使用互斥同步来保证就失去意义了。

这类乐观锁指令常见的有：
* 测试并设置（Test-and-Test）
* 获取并增加（Fetch-and-Increment）
* 交换（Swap）
* 比较并交换（CAS）
* 加载链接、条件存储（Load-linked/Store-Conditional）

Java 典型应用场景：J.U.C 包中的原子类（基于 Unsafe 类的 CAS 操作）

###### 无同步

要保证线程安全，不一定非要进行同步。同步只是保证共享数据争用时的正确性，如果一个方法本来就不涉及共享数据，那么自然无须同步。
* 可重入代码：如果一个方法，它的返回结果是可以预测，即只要输入了相同的数据，就能返回相同的数据结果。
* 线程本地存储-ThreadLocal为共享变量在每个线程中都创建一个本地副本，这个副本只能被当前线程访问其他线程无法访问，那么自然是线程安全的（这句话有问题）

### 活跃性问题
*** 
#### 死锁（DeadLock）
多线程相互等待对方释放锁。
死锁适当线程进入无限期等待状态时发生的情况，因为锁清秋的锁被另一个线程持有，而另一个线程又等待一个线程持有的另一个锁。
![c9ebdd24afea794a052aac6715774345.png](en-resource://database/786:1)
##### 避免死锁
1. 按序枷锁
   当多个线程需要相同的一些锁，但是按照不同的顺序加速偶，死锁就容易发生。如果能确保所有的线程都是按照相同的顺序获得锁，那么死锁就不会发生。按照顺序加锁是一种有效的死锁预防机制。但是，这种方式需要你事先知道所有可能会用到的锁(译者注：并对这些锁做适当的排序)，但总有些时候是无法预知的。
2. 超时释放锁
   另一个可以避免死锁的方法是尝试获取所得时候加一个超时时间，这也就意味着在尝试获取锁的过程中若超过了这个时限该线程则放弃对该锁请求。若一个线程没有在给定的时限内成功获得所有需要的锁，则会进行回退并释放所有已经获得的锁，然后等待一段随机的时间再重试。这段随机的等待时间让其它线程有机会尝试获取相同的这些锁，并且让该应用在没有获得锁的时候可以继续运行(译者注：加锁超时后可以先继续运行干点其它事情，再回头来重复之前加锁的逻辑)。
3. 死锁检测
   死锁检测是一个更好的死锁预防机制，它主要是针对那些不可能实现按序加锁并且锁超时也不可行的场景。每当一个线程获得了锁，会在线程和锁相关的数据结构中（map、graph 等等）将其记下。除此之外，每当有线程请求锁，也需要记录在这个数据结构中。当一个线程请求锁失败时，这个线程可以遍历锁的关系图看看是否有死锁发生。

如果检测出死锁，有两种处理手段：
* 释放所有锁：回退，并且等待一段随机的时间后重试。这个和简单的加锁超时类似，不一样的是只有死锁已经发生了才回退，而不会是因为加锁的请求超时了。虽然有回退和等待，但是如果有大量的线程竞争同一批锁，它们还是会重复地死锁（编者注：原因同超时类似，不能从根本上减轻竞争）。
* 一个更好的方案是给这些线程设置优先级，让一个（或几个）线程回退，剩下的线程就像没发生死锁一样继续保持着它们需要的锁。如果赋予这些线程的优先级是固定不变的，同一批线程总是会拥有更高的优先级。为避免这个问题，可以在死锁发生的时候设置随机的优先级。
#### 活锁

活锁是一个递归的情况，两个或更多的线程会不断重复一个特定的代码逻辑。预期的逻辑通常为其他线程提供机会继续支持'this'线程。想象这样一个例子：两个人在狭窄的走廊里相遇，二者都很礼貌，试图移到旁边让对方先通过。但是他们最终在没有取得任何进展的情况下左右摇摆，因为他们都在同一时间向相同的方向移动。
![3a32ed833adb05d1e95b8c8a191e8f84.png](en-resource://database/788:1)

如图所示：两个线程想要通过一个 Worker 对象访问共享公共资源的情况，但是当他们看到另一个 Worker（在另一个线程上调用）也是“活动的”时，它们会尝试将该资源交给其他工作者并等待为它完成。如果最初我们让两名工作人员都活跃起来，他们将会面临活锁问题。

##### 避免活锁

解决“活锁”的方案很简单，谦让时，尝试等待一个随机的时间就可以了。由于等待的时间是随机的，所以同时相撞后再次相撞的概率就很低了。“等待一个随机时间”的方案虽然很简单，却非常有效，Raft 这样知名的分布式一致性算法中也用到了它。


#### 饥饿（Starvation）
* 高优先级线程吞噬所有的低优先级现成的cpu时间。
* 线程备用就读塞在一个等待进入同步快的状态，因为其他线程总是能在他之前持续地对该同步块进行访问。
* 线程在等待一个本身（在骑上调用wait()）也处于永久等待完成的对象，因为其他线程总是被持续地获得唤醒。


Java 不可能实现 100% 的公平性，我们依然可以通过同步结构在线程间实现公平性的提高。有三种方案：保证资源充足公平地分配资源避免持有锁的线程长时间执行这三个方案中，方案一和方案三的适用场景比较有限，因为很多场景下，资源的稀缺性是没办法解决的，持有锁的线程执行的时间也很难缩短。倒是方案二的适用场景相对来说更多一些。

### 性能问题
***
#### 上下文切换

当 CPU 从执行一个线程切换到执行另一个线程时，CPU 需要保存当前线程的本地数据，程序指针等状态，并加载下一个要执行的线程的本地数据，程序指针等。这个开关被称为“上下文切换”。

* 无锁并发编程 - 多线程竞争锁时，会引起上下文切换，所以多线程处理数据时，可以用一些办法来避免使用锁，如将数据的 ID 按照 Hash 算法取模分段，不同的线程处理不同段的数据。
* CAS 算法 - Java 的 Atomic 包使用 CAS 算法来更新数据，而不需要加锁。
* 使用最少线程 - 避免创建不需要的线程，比如任务很少，但是创建了很多线程来处理，这样会造成大量线程都处于等待状态。
* 使用协程 - 在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换。

##### 资源限制

资源限制是指在进行并发编程时，程序的执行速度受限于计算机硬件资源或软件资源。

在并发编程中，将代码执行速度加快的原则是将代码中串行执行的部分变成并发执行，但是如果将某段串行的代码并发执行，因为受限于资源，仍然在串行执行，这时候程序不仅不会加快执行，反而会更慢，因为增加了上下文切换和资源调度的时间。

### 小结
***
并发编程可以总结为三个核心问题：分工、同步、互斥。

* 分工：是指如何高效地拆解任务并分配给线程
* 同步：是指线程之间如何协作
* 互斥：是指保证同一时刻只允许一个线程访问共享资源

### 参考资料
* [Java并发编程实战](https://book.douban.com/subject/10484692/)
* [《Java并发编程的艺术》](https://book.douban.com/subject/26591326/)
* [《深入理解Java虚拟机》](https://book.douban.com/subject/34907497/)
* http://tutorials.jenkov.com/java-concurrency/benefits.html
* https://www.logicbig.com/tutorials/core-java-tutorial/java-multi-threading/thread-deadlock.html
* https://www.logicbig.com/tutorials/core-java-tutorial/java-multi-threading/thread-livelock.html
* https://www.logicbig.com/tutorials/core-java-tutorial/java-multi-threading/thread-starvation.html
## Happens-Before原则

* 管程锁定规则：对于同一个锁，如果一个 unlock 操作先行发生于一个 lock 操作，那么该 unlock 操作（包括 unlock 操作之前的操作）所产生的影响对于该 lock 操作（包括 lock 操作之后的操作）是可见的。
* volatile 变量规则：对于同一个 volatile 变量，如果对于这个变量的写操作先行发生于对于这个变量的读操作，那么对于这个变量的写操作（包括写操作之前的操作）所产生的影响对于这个变量的读操作（包括读操作之后的操作）是可见的。
* 线程启动规则：对于同一个 Thread 对象，该 Thread 对象的 start()方法先行发生于此线程的每一个动作，也就是说对线程 start()方法调用（包括 start 方法之前的操作）所产生的影响对于该该线程的每一个动作都是可见的。
* 线程终止规则：对于一个线程，线程中发生的所有操作先行发生于对此线程的终止检测，也就是说线程中的所有操作所产生的影响对于调用线程的 Thread.join()方法或者 Thread.isAlive()方法（包括调用这两个方法之后的操作）都是可见的。  * 线程中断规则：对于同一个线程，对线程 interrupt()方法的调用先行发生于该线程检测到中断事件的发生，也就是说线程 interrupt()方法调用（包括 interrupt 方法调用之前的操作）所产生的影响对于该线程检测到中断事件（包括检测到中断事件之后的操作）是可见的。
* 对象终结规则：对于同一个对象，它的构造方法执行结束先行发生于它的 finalize()方法的开始，也就是说一个对象的构造方法结束（包括构造方法结束前的操作）所产生的影响，对于它的 finalize()方法开始执行（包括开始之后的操作）是可见的。



## Java线程基础
## Java并发核心机制
## Java锁
## Java原子类
## Java并发和容器
## Java线程池
## Java并发工具类
## Java内存模型
## ForkJoin框架
## Synchronized
